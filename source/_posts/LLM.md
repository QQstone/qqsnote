---
title: LLM
date: 2025-03-05 14:42:14
tags:
---
#### Token
Token 是大型语言模型处理文本的基本单位。不同模型采用不同的分词方式(分词器)，当我们将一段文字输入模型时，模型会先将其拆解成 Token 序列，然后通过这些序列进行预测。

分词器的具体执行原理较为复杂。我们可以简化地将分词器视为一个字典，对于字典中存在的内容，按照字典进行切分并替换为对应的数字
Hello 你好呀 --> Hello | _ | 你好 | 呀 (character: 9, token: 4)

#### 神经网络结构
经典的卷积神经网络(CNN)结构包括以下层：

输入层→卷积层→池化层→全连接层

例如，假设要对0-9十个数字的图片进行分类，则全连接层的输出维度通常要设置为10，与输出分类一一对应。最后通过Softmax层，程序就可以将输出转换为每个类别的概率，概率最高的类别即为预测结果

理解了CNN的输出层维度与分类任务的关系，我们也可以将这个概念迁移到对Transformer语言模型的理解上。尽管二者架构迥异，但是它们在输出层维度及分类任务的原理上仍有共性。

语言模型本质上仍然是一种预测模型，它的核心目标，就是预测给定上下文的下一token。输入层在复杂的网络中传播，最终到达输出层，并且形成 N 个维度的输出。而每个维度正对应着分词表中的一个token。

例如，如果分词表包含50000个token，那么Transformer模型的输出层将输出一个50000维的向量，向量中的每个元素代表对应token的概率。模型会选择概率最高的token作为预测的下一个token。

> 早期llm的词表(Vocabulary)规模非常小，对应的输出维度也很小，因此相同环境下的理论计算性能更优。但是词表中没有出现的罕见词，有可能被拆分成子词甚至字符。就如ChatGPT 4.0及以前的Vocabulary，大部分中文字符，都会被拆分为2-3个子词token。显然，小规模的Vocabulary有更低的信息密度，相同的输入输出会占据更大的序列长度，这对于输入与输出都有很大的负面影响。

> 以Llama2-7B模型为例，当词表从32k扩展至64k时，理论的嵌入矩阵参数量从32,000×4096增至65,536×4096，但平均序列长度缩短约30%，实际运行中的整体推理速度提升了15%。

#### Transformer
Transformer语言模型是基于自注意力机制构建的深度学习架构，已在自然语言处理领域引发革命性变革。
核心架构：
+ 自注意力机制：通过计算序列中每个元素与其他元素的相关性权重（Q/K/V矩阵），捕捉长距离依赖关系。相比RNN/CNN，并行计算效率提升3-8倍（论文7、10）。

+ 多头注意力：将输入切分为多个子空间并行处理，可同时学习语法、语义等不同特征维度（论文3、12）。

+ 位置编码：采用正弦函数生成绝对位置编码（论文1）或相对位置编码（论文18），解决序列顺序建模问题。

它的核心目标，就是预测给定上下文的下一token。

这意味着，Transformer语言模型每输出一个token，输入数据便在神经网络经过了一次完整的传播。最终从词表查询到token string，展示在你眼前的，无论是一个字母，一个单词，甚至一个短语，从模型的角度来看，它们都是“平等的”——从模型角度来看，它们都是一个独立的预测单元，最终对应到词表中的一个 token。

从输入角度上看，正如我们之前提到的，transformer的自注意力模型会将上下文加载到注意力矩阵中，上下文token数直接决定了矩阵的规模，由于O(n²)的时间复杂度，以及O(n²d)的空间复杂度（d为隐藏层维度），小规模词表所带来的低密度信息，可能反而削弱计算性能，增加资源消耗。

从输出角度上看，更小的词表同样意味着单位token包含的信息更少，完成同样的一句输出将会经过更多的推理次数。